# Word2vec_tutorials
The way I learnt word2vec. I suggest to read in the same order the references are put across.

# Jay Alammar's non-technical introduction to word2vec
http://jalammar.github.io/illustrated-word2vec/
This is the best word2vec 101 I have come across. One gets to learn word2vec with very little understanding of the machine learning/complexities underneath

# Chris McCormick's indepth tutorial on word2vec (skipgram only)
http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/
Shows the math underneath in a minimalistic yet in practical way

# Detailed explanation on Word embeddings (Super Technical)
http://ruder.io/word-embeddings-1/index.html
Sebastian Ruder does a detailed walk through and covers the variations

# Google's trained Word2Vec model in Python
http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/





# Word2vec original paper
1. Efficient Estimation of Word Representations in
Vector Space : https://arxiv.org/pdf/1301.3781.pdf
2. Distributed Representations of Words and Phrases
and their Compositionality : https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf

